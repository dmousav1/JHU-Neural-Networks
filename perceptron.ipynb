{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, weights, desired_output, bias, eta):  \n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = bias\n",
    "        self.desired_output = desired_output\n",
    "        self.eta = eta\n",
    "        self.delta_weights = np.zeros(len(weights))\n",
    "        self.delta_bias = 0.0\n",
    "        self.activity = 0.0\n",
    "        self.activation = 0.0\n",
    "        self.delta = 0.0\n",
    "\n",
    "    # Calculate activity\n",
    "    def calc_activity(self, input):\n",
    "        input = np.array(input)\n",
    "        self.activity = self.bias + np.dot(self.weights, input)\n",
    "    \n",
    "    # Sigmoid activation function\n",
    "    def calc_activation(self):\n",
    "        self.activation = 1 / (1 + np.exp(-self.activity))\n",
    "        \n",
    "    # Calculate delta and set delta weights and bias\n",
    "    def set_delta_weights(self, input):\n",
    "        input = np.array(input)\n",
    "        activation_derivative = self.activation * (1 - self.activation)  # Derivative of sigmoid\n",
    "        self.delta = (self.activation - self.desired_output) * activation_derivative\n",
    "        self.delta_weights = self.eta * self.delta * input\n",
    "        self.delta_bias = self.eta * self.delta\n",
    "    \n",
    "    # Update weights and bias using delta\n",
    "    def update_weights(self):\n",
    "        self.weights = self.weights - self.delta_weights\n",
    "        self.bias = self.bias - self.delta_bias\n",
    "    \n",
    "    # Train the Perceptron for a certain number of iterations\n",
    "    def train(self, input, iterations):\n",
    "        for i in range(iterations):\n",
    "            # Calculate activity and activation. Set delta weights & bias. Update weights and bias. Print each iteration.\n",
    "            self.calc_activity(input)\n",
    "            self.calc_activation()\n",
    "            self.set_delta_weights(input)\n",
    "            self.update_weights()\n",
    "            print(f\"Iteration {i+1}: Weights = {self.weights}, Bias = {self.bias}, Activation = {self.activation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7279011823597308\n",
      "Iteration 1: Weights = [0.4159565  1.07795106], Bias = 0.2199456263262151, Activation = 0.7279011823597308\n",
      "Iteration 2: Weights = [0.49182389 1.16330188], Bias = 0.31477986352070136, Activation = 0.8209602413294873\n",
      "Iteration 3: Weights = [0.54077933 1.21837675], Bias = 0.3759741655868892, Activation = 0.8526088146349863\n",
      "Iteration 4: Weights = [0.57664676 1.2587276 ], Bias = 0.42080844882530266, Activation = 0.870471795515964\n",
      "Iteration 5: Weights = [0.60473049 1.2903218 ], Bias = 0.4559131085310897, Activation = 0.8823610045092689\n",
      "Iteration 6: Weights = [0.6276511  1.31610748], Bias = 0.4845638694875073, Activation = 0.890999084228504\n",
      "Iteration 7: Weights = [0.64689945 1.33776188], Bias = 0.5086243095202532, Activation = 0.8976315827556391\n",
      "Iteration 8: Weights = [0.66340536 1.35633103], Bias = 0.5292567025750439, Activation = 0.9029227158190951\n",
      "Iteration 9: Weights = [0.67778782 1.3725113 ], Bias = 0.5472347807626515, Activation = 0.9072641843237704\n",
      "Iteration 10: Weights = [0.69047954 1.38678949], Bias = 0.5630994292235433, Activation = 0.9109041736742518\n",
      "Iteration 11: Weights = [0.70179476 1.3995191 ], Bias = 0.5772434489396177, Activation = 0.914008671700023\n",
      "Iteration 12: Weights = [0.71196877 1.41096487], Bias = 0.5899609634531847, Activation = 0.9166934887704334\n",
      "Iteration 13: Weights = [0.72118225 1.42133003], Bias = 0.6014778145511714, Activation = 0.9190422261110651\n",
      "Iteration 14: Weights = [0.72957688 1.43077399], Bias = 0.61197110049122, Activation = 0.9211169508566813\n",
      "Iteration 15: Weights = [0.73726576 1.43942398], Bias = 0.6215822005719889, Activation = 0.9229648397589835\n",
      "Iteration 16: Weights = [0.74434058 1.44738316], Bias = 0.6304257284082669, Activation = 0.9246224768114605\n",
      "Iteration 17: Weights = [0.75087668 1.45473627], Bias = 0.6385958505161231, Activation = 0.926118725569756\n",
      "Iteration 18: Weights = [0.75693668 1.46155376], Bias = 0.6461708482489813, Activation = 0.9274767033922389\n",
      "Iteration 19: Weights = [0.76257318 1.46789483], Bias = 0.6532164777525273, Activation = 0.9287151717682731\n",
      "Iteration 20: Weights = [0.76783079 1.47380964], Bias = 0.6597884885522179, Activation = 0.9298495364962199\n",
      "Iteration 21: Weights = [0.77274763 1.47934109], Bias = 0.6659345412292302, Activation = 0.9308925808530224\n",
      "Iteration 22: Weights = [0.77735655 1.48452612], Bias = 0.6716956881557085, Activation = 0.9318550121102206\n",
      "Iteration 23: Weights = [0.78168603 1.48939678], Bias = 0.6771075313653899, Activation = 0.9327458750745028\n",
      "Iteration 24: Weights = [0.78576091 1.49398102], Bias = 0.6822011383646351, Activation = 0.933572869269555\n",
      "Iteration 25: Weights = [0.78960302 1.4983034 ], Bias = 0.6870037740601985, Activation = 0.934342595212611\n",
      "Iteration 26: Weights = [0.79323159 1.50238554], Bias = 0.6915394913118388, Activation = 0.9350607477832276\n",
      "Iteration 27: Weights = [0.79666369 1.50624665], Bias = 0.6958296115911216, Activation = 0.9357322696086261\n",
      "Iteration 28: Weights = [0.7999145  1.50990381], Bias = 0.6998931193512974, Activation = 0.9363614738790155\n",
      "Iteration 29: Weights = [0.80299759 1.51337229], Bias = 0.703746988009649, Activation = 0.9369521435386683\n",
      "Iteration 30: Weights = [0.80592516 1.51666581], Bias = 0.7074064512614995, Activation = 0.9375076120392815\n",
      "Iteration 31: Weights = [0.80870818 1.51979671], Bias = 0.7108852303425837, Activation = 0.9380308295714458\n",
      "Iteration 32: Weights = [0.81135658 1.52277615], Bias = 0.7141957255300584, Activation = 0.9385244177610088\n",
      "Iteration 33: Weights = [0.81387934 1.52561426], Bias = 0.7173491784104138, Activation = 0.9389907151302141\n",
      "Iteration 34: Weights = [0.81628465 1.52832023], Bias = 0.7203558100955089, Activation = 0.9394318151102985\n",
      "Iteration 35: Weights = [0.81857995 1.53090245], Bias = 0.7232249395291559, Activation = 0.9398495980050562\n",
      "Iteration 36: Weights = [0.82077207 1.53336858], Bias = 0.7259650852190281, Activation = 0.9402457580100947\n",
      "Iteration 37: Weights = [0.82286724 1.53572565], Bias = 0.7285840530959007, Activation = 0.9406218261661818\n",
      "Iteration 38: Weights = [0.82487121 1.53798011], Bias = 0.7310890127029067, Activation = 0.9409791899498805\n",
      "Iteration 39: Weights = [0.82678925 1.54013791], Bias = 0.7334865635207867, Activation = 0.9413191100680461\n",
      "Iteration 40: Weights = [0.82862623 1.54220451], Bias = 0.7357827929178993, Activation = 0.9416427349154591\n",
      "Iteration 41: Weights = [0.83038666 1.54418499], Bias = 0.7379833269585979, Activation = 0.941951113070004\n",
      "Iteration 42: Weights = [0.8320747  1.54608404], Bias = 0.7400933750971219, Activation = 0.9422452041323063\n",
      "Iteration 43: Weights = [0.83369422 1.54790599], Bias = 0.7421177696162168, Activation = 0.9425258881626832\n",
      "Iteration 44: Weights = [0.8352488 1.5496549], Bias = 0.7440610005323749, Activation = 0.9427939739247567\n",
      "Iteration 45: Weights = [0.8367418  1.55133452], Bias = 0.7459272465767574, Activation = 0.9430502061098602\n",
      "Iteration 46: Weights = [0.83817632 1.55294836], Bias = 0.7477204027677059, Activation = 0.9432952716877138\n",
      "Iteration 47: Weights = [0.83955528 1.55449969], Bias = 0.7494441050135141, Activation = 0.9435298055054189\n",
      "Iteration 48: Weights = [0.8408814  1.55599158], Bias = 0.7511017521198055, Activation = 0.9437543952375795\n",
      "Iteration 49: Weights = [0.84215722 1.55742687], Bias = 0.7526965255220837, Activation = 0.943969585774477\n",
      "Iteration 50: Weights = [0.84338513 1.55880827], Bias = 0.7542314070188774, Activation = 0.9441758831220759\n",
      "Iteration 51: Weights = [0.84456736 1.56013828], Bias = 0.7557091947428674, Activation = 0.9443737578766896\n",
      "Iteration 52: Weights = [0.84570601 1.56141927], Bias = 0.7571325175752284, Activation = 0.944563648327993\n",
      "Iteration 53: Weights = [0.84680308 1.56265346], Bias = 0.7585038481811327, Activation = 0.944745963236408\n",
      "Iteration 54: Weights = [0.84786041 1.56384296], Bias = 0.7598255148211436, Activation = 0.9449210843244409\n",
      "Iteration 55: Weights = [0.84887977 1.56498974], Bias = 0.7610997120733946, Activation = 0.9450893685161027\n",
      "Iteration 56: Weights = [0.84986281 1.56609566], Bias = 0.7623285105844723, Activation = 0.9452511499539388\n",
      "Iteration 57: Weights = [0.85081109 1.56716248], Bias = 0.7635138659523305, Activation = 0.9454067418192725\n",
      "Iteration 58: Weights = [0.8517261  1.56819186], Bias = 0.7646576268320007, Activation = 0.9455564379779299\n",
      "Iteration 59: Weights = [0.85260923 1.56918539], Bias = 0.765761542344012, Activation = 0.9457005144708589\n",
      "Iteration 60: Weights = [0.85346182 1.57014454], Bias = 0.7668272688560377, Activation = 0.9458392308666062\n",
      "Iteration 61: Weights = [0.8542851  1.57107074], Bias = 0.7678563762001266, Activation = 0.9459728314905174\n",
      "Iteration 62: Weights = [0.85508028 1.57196532], Bias = 0.7688503533807794, Activation = 0.946101546543706\n",
      "Iteration 63: Weights = [0.85584849 1.57282955], Bias = 0.7698106138229396, Activation = 0.9462255931232775\n",
      "Iteration 64: Weights = [0.8565908  1.57366465], Bias = 0.7707385002035521, Activation = 0.9463451761539311\n",
      "Iteration 65: Weights = [0.85730823 1.57447176], Bias = 0.7716352889056038, Activation = 0.9464604892398906\n",
      "Iteration 66: Weights = [0.85800176 1.57525197], Bias = 0.7725021941293932, Activation = 0.9465717154450838\n",
      "Iteration 67: Weights = [0.8586723  1.57600633], Bias = 0.7733403716921128, Activation = 0.9466790280086007\n",
      "Iteration 68: Weights = [0.85932074 1.57673583], Bias = 0.7741509225435996, Activation = 0.9467825910016772\n",
      "Iteration 69: Weights = [0.85994792 1.57744141], Bias = 0.7749348960232555, Activation = 0.9468825599317648\n",
      "Iteration 70: Weights = [0.86055463 1.57812396], Bias = 0.7756932928806133, Activation = 0.9469790822986502\n",
      "Iteration 71: Weights = [0.86114165 1.57878436], Bias = 0.776427068079789, Activation = 0.9470722981070541\n",
      "Iteration 72: Weights = [0.86170971 1.57942342], Bias = 0.7771371334060705, Activation = 0.9471623403396802\n",
      "Iteration 73: Weights = [0.86225949 1.58004192], Bias = 0.7778243598911271, Activation = 0.9472493353942674\n",
      "Iteration 74: Weights = [0.86279166 1.58064062], Bias = 0.7784895800717474, Activation = 0.947333403487839\n",
      "Iteration 75: Weights = [0.86330687 1.58122023], Bias = 0.7791335900956071, Activation = 0.9474146590310214\n"
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "weights = [0.24, 0.88]\n",
    "bias = 0.0\n",
    "eta = 5.0\n",
    "desired_output = 0.95\n",
    "iterations = 75\n",
    "\n",
    "# Set Perceptron\n",
    "perceptron = Perceptron(weights, desired_output, bias, eta) # type: ignore\n",
    "\n",
    "# Set Inputs and Calculate Acitivity; Calculate Activation\n",
    "inputs = [0.8, 0.9]\n",
    "perceptron.calc_activity(inputs)\n",
    "perceptron.calc_activation()\n",
    "print(perceptron.activation)\n",
    "\n",
    "# Question 2\n",
    "perceptron.train(inputs, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "weights = [0.24, 0.88]\n",
    "bias = 0.0\n",
    "eta = 5.0\n",
    "desired_output = 0.15\n",
    "iterations = 30\n",
    "\n",
    "# Set Perceptron\n",
    "perceptron = Perceptron(weights, desired_output, bias, eta) # type: ignore\n",
    "\n",
    "# Set Inputs and Calculate Acitivity; Calculate Activation\n",
    "inputs = [0.8, 0.9]\n",
    "perceptron.train(inputs, iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
